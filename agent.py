import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import yaml
import logging
import pickle
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
with open('config.yaml', 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

# ==========================
# ü§ñ –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ê–ö–¢–û–†-–ö–†–ò–¢–ò–ö
# ==========================
class ActorCriticLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        super(ActorCriticLSTM, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.actor = nn.Linear(hidden_dim, output_dim)
        self.critic = nn.Linear(hidden_dim, 1)
    
    def forward(self, x):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∏–∑–º–µ–Ω—è–µ–º —Ñ–æ—Ä–º—É –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞
        if len(x.shape) == 2:  # –ï—Å–ª–∏ (batch_size, input_size)
            x = x.unsqueeze(1)  # –î–æ–±–∞–≤–ª—è–µ–º seq_length = 1

        lstm_out, _ = self.lstm(x)
        lstm_out = lstm_out[:, -1, :]  # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        policy = F.softmax(self.actor(lstm_out), dim=-1)
        value = self.critic(lstm_out)
        return policy, value

# ==========================
# üõ†Ô∏è –ê–ì–ï–ù–¢
# ==========================
class Agent:
    def __init__(self, state_dim, action_dim):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_dim = config['model']['hidden_size']
        self.num_layers = config['model']['num_layers']
        self.gamma = config['training']['gamma']
        self.lr = config['training']['learning_rate']
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = ActorCriticLSTM(self.state_dim, self.hidden_dim, self.action_dim, self.num_layers).to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
    
    def select_action(self, state):
        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        policy, _ = self.model(state)
        action = torch.multinomial(policy, 1).item()
        return action
    
    def update(self, rewards, log_probs, values, dones):
        returns = []
        discounted_sum = 0
        
        for reward, done in zip(reversed(rewards), reversed(dones)):
            if done:
                discounted_sum = 0
            discounted_sum = reward + (self.gamma * discounted_sum)
            returns.insert(0, discounted_sum)
        
        returns = torch.FloatTensor(returns).to(self.device)
        log_probs = torch.stack(log_probs)
        values = torch.stack(values)
        
        advantage = returns - values.squeeze()
        actor_loss = -(log_probs * advantage).mean()
        critic_loss = F.mse_loss(values.squeeze(), returns)
        loss = actor_loss + critic_loss
        
        self.optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(self.model.parameters(), config['training']['clip_grad_norm'])
        self.optimizer.step()
        logger.info(f"üîÑ –ê–∫—Ç–æ—Ä-–ª–æ—Å—Å: {actor_loss.item():.4f}, –ö—Ä–∏—Ç–∏–∫-–ª–æ—Å—Å: {critic_loss.item():.4f}")
    
    def save_model(self, path):
        torch.save(self.model.state_dict(), path)
        logger.info(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {path}")
    
    def load_model(self, path):
        self.model.load_state_dict(torch.load(path, map_location=self.device))
        logger.info(f"üì• –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —Ñ–∞–π–ª–∞: {path}")

# ==========================
# üöÄ –û–ë–£–ß–ï–ù–ò–ï –ê–ì–ï–ù–¢–ê
# ==========================
COLOR_MAP = {'–∑–µ–ª–µ–Ω—ã–π': 0, '—Å–∏–Ω–∏–π': 1, '–∑–æ–ª–æ—Ç–æ–π': 2}

def encode_sequence(sequence):
    """–ö–æ–¥–∏—Ä—É–µ—Ç —Å—Ç—Ä–æ–∫–æ–≤—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ü–≤–µ—Ç–æ–≤ –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç."""
    return [COLOR_MAP[color] for color in sequence if color in COLOR_MAP]

def evaluate_model(agent, training_data):
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ä–µ–¥–Ω—é—é –Ω–∞–≥—Ä–∞–¥—É.
    """
    if not training_data:
        logger.warning("–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç. –ú–µ—Ç—Ä–∏–∫–∞ –Ω–µ –≤—ã—á–∏—Å–ª–µ–Ω–∞.")
        return float('-inf')

    training_data_encoded = encode_sequence(training_data)
    total_reward = 0

    for color in training_data_encoded:
        state = torch.FloatTensor([[color]]).to(agent.device)  # seq_length = 1
        action = agent.select_action([color])
        reward = 1.0 if action == color else -1.0
        total_reward += reward

    avg_reward = total_reward / len(training_data_encoded)
    logger.info(f"–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {avg_reward:.4f}")
    return avg_reward

def train_agent(agent, training_data):
    logger.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞.")

    # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –æ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    model_path = config['general']['save_model_path']
    best_metric = float('-inf')  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ö—É–¥—à–µ–π –≤–æ–∑–º–æ–∂–Ω–æ–π –º–µ—Ç—Ä–∏–∫–æ–π

    if os.path.exists(model_path):
        agent.load_model(model_path)
        logger.info(f"üì• –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å –∏–∑ {model_path}.")
        best_metric = evaluate_model(agent, training_data)  # –û—Ü–µ–Ω–∫–∞ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        logger.info(f"üîÑ –ú–µ—Ç—Ä–∏–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {best_metric:.4f}")
    else:
        logger.info("üì• –°–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è.")

    if not training_data:
        logger.warning("–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç. –î–æ–±–∞–≤—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ–ø—ã—Ç–∫—É.")
        return

    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {len(training_data)} –∑–∞–ø–∏—Å–µ–π.")
    training_data_encoded = encode_sequence(training_data)

    for epoch in range(config['training']['epochs']):
        logger.info(f"–≠–ø–æ—Ö–∞ {epoch + 1}/{config['training']['epochs']} –Ω–∞—á–∞–ª–∞—Å—å.")
        log_probs, values, rewards, dones = [], [], [], []

        epoch_rewards = 0  # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥—ã –∑–∞ —ç–ø–æ—Ö—É

        for color in training_data_encoded:
            # –û–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ñ–æ—Ä–º—ã –¥–∞–Ω–Ω—ã—Ö
            state = torch.FloatTensor([[color]]).to(agent.device)  # seq_length = 1
            action = agent.select_action([color])
            reward = 1.0 if action == color else -1.0
            done = False

            epoch_rewards += reward  # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥—ã

            policy, value = agent.model(state)
            log_prob = torch.log(policy.squeeze(0)[action])

            log_probs.append(log_prob)
            values.append(value)
            rewards.append(reward)
            dones.append(done)

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
        agent.update(rewards, log_probs, values, dones)

        # –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ —ç–ø–æ—Ö—É
        avg_reward = epoch_rewards / len(training_data_encoded)
        logger.info(f"–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ —ç–ø–æ—Ö—É {epoch + 1}: {avg_reward:.4f}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if avg_reward > best_metric:
            best_metric = avg_reward
            agent.save_model(model_path)
            logger.info(f"üíæ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å –º–µ—Ç—Ä–∏–∫–æ–π: {best_metric:.4f}")

        if (epoch + 1) % config['metrics']['log_interval'] == 0:
            logger.info(f"–≠–ø–æ—Ö–∞ {epoch + 1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    logger.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")


if __name__ == '__main__':
    agent = Agent(state_dim=1, action_dim=3)
    logger.info("–ê–≥–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
    
    with open(config['paths']['training_data'], 'rb') as f:
        training_data = pickle.load(f)
    train_agent(agent, training_data)
