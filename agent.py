import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import yaml
import logging
import pickle
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
with open('config.yaml', 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)


# ==========================
# ü§ñ –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ê–ö–¢–û–†-–ö–†–ò–¢–ò–ö
# ==========================
class ActorCriticLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        super(ActorCriticLSTM, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.actor = nn.Linear(hidden_dim, output_dim)
        self.critic = nn.Linear(hidden_dim, 1)
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        lstm_out = lstm_out[:, -1, :]  # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        policy = F.softmax(self.actor(lstm_out), dim=-1)
        value = self.critic(lstm_out)
        return policy, value


# ==========================
# üõ†Ô∏è –ê–ì–ï–ù–¢
# ==========================
class Agent:
    def __init__(self, state_dim, action_dim):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_dim = config['model']['hidden_size']
        self.num_layers = config['model']['num_layers']
        self.gamma = config['training']['gamma']
        self.lr = config['training']['learning_rate']
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        self.model = ActorCriticLSTM(self.state_dim, self.hidden_dim, self.action_dim, self.num_layers).to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
    
    def select_action(self, state):
        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        policy, _ = self.model(state)
        action = torch.multinomial(policy, 1).item()
        return action
    
    def update(self, rewards, log_probs, values, dones):
        returns = []
        discounted_sum = 0
        
        for reward, done in zip(reversed(rewards), reversed(dones)):
            if done:
                discounted_sum = 0
            discounted_sum = reward + (self.gamma * discounted_sum)
            returns.insert(0, discounted_sum)
        
        returns = torch.FloatTensor(returns).to(self.device)
        log_probs = torch.stack(log_probs)
        values = torch.stack(values)
        
        advantage = returns - values.squeeze()
        
        actor_loss = -(log_probs * advantage).mean()
        critic_loss = F.mse_loss(values.squeeze(), returns)
        loss = actor_loss + critic_loss
        
        self.optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(self.model.parameters(), config['training']['clip_grad_norm'])
        self.optimizer.step()
        
        logger.info(f"üîÑ –ê–∫—Ç–æ—Ä-–ª–æ—Å—Å: {actor_loss.item():.4f}, –ö—Ä–∏—Ç–∏–∫-–ª–æ—Å—Å: {critic_loss.item():.4f}")
    
    def save_model(self, path):
        torch.save(self.model.state_dict(), path)
        logger.info(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {path}")
    
    def load_model(self, path):
        self.model.load_state_dict(torch.load(path, map_location=self.device))
        logger.info(f"üì• –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —Ñ–∞–π–ª–∞: {path}")


# ==========================
# üöÄ –û–ë–£–ß–ï–ù–ò–ï –ê–ì–ï–ù–¢–ê
# ==========================
COLOR_MAP = {'–∑–µ–ª–µ–Ω—ã–π': 0, '—Å–∏–Ω–∏–π': 1, '–∑–æ–ª–æ—Ç–æ–π': 2}  # –ö–∞—Ä—Ç–∞ —Ü–≤–µ—Ç–æ–≤ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è

def encode_sequence(sequence):
    """
    –ö–æ–¥–∏—Ä—É–µ—Ç —Å—Ç—Ä–æ–∫–æ–≤—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ü–≤–µ—Ç–æ–≤ –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç.
    """
    return [COLOR_MAP[color] for color in sequence if color in COLOR_MAP]

def train_agent(agent, training_data):
    logger.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞.")
    
    if not training_data:
        logger.warning("–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç. –î–æ–±–∞–≤—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ–ø—ã—Ç–∫—É.")
        return

    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {len(training_data)} –∑–∞–ø–∏—Å–µ–π.")
    training_data_encoded = [encode_sequence([color]) for color in training_data]

    for epoch in range(config['training']['epochs']):
        logger.info(f"–≠–ø–æ—Ö–∞ {epoch + 1}/{config['training']['epochs']} –Ω–∞—á–∞–ª–∞—Å—å.")
        log_probs, values, rewards, dones = [], [], [], []
        
        for sequence in training_data_encoded:
            if not sequence:  # –ü—Ä–æ–ø—É—Å–∫ –ø—É—Å—Ç—ã—Ö –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø—É—Å—Ç–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –ü—Ä–æ–ø—É—Å–∫.")
                continue
            
            state = torch.FloatTensor(sequence).to(agent.device)
            action = agent.select_action(state)
            
            # –ü—Ä–∏–º–µ—Ä –Ω–∞–≥—Ä–∞–¥—ã
            reward = 1.0  
            done = False  

            policy, value = agent.model(state.unsqueeze(0))
            log_prob = torch.log(policy.squeeze(0)[action])
            
            log_probs.append(log_prob)
            values.append(value)
            rewards.append(reward)
            dones.append(done)
        
        agent.update(rewards, log_probs, values, dones)
        
        if (epoch + 1) % config['metrics']['log_interval'] == 0:
            logger.info(f"–≠–ø–æ—Ö–∞ {epoch + 1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    agent.save_model(config['general']['save_model_path'])
    logger.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
def train_agent(agent, training_data):
    logger.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞.")
    
    if not training_data:
        logger.warning("–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç. –î–æ–±–∞–≤—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ–ø—ã—Ç–∫—É.")
        return

    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {len(training_data)} –∑–∞–ø–∏—Å–µ–π.")
    training_data_encoded = [encode_sequence([color]) for color in training_data]

    for epoch in range(config['training']['epochs']):
        logger.info(f"–≠–ø–æ—Ö–∞ {epoch + 1}/{config['training']['epochs']} –Ω–∞—á–∞–ª–∞—Å—å.")
        log_probs, values, rewards, dones = [], [], [], []
        
        for sequence in training_data_encoded:
            if not sequence:  # –ü—Ä–æ–ø—É—Å–∫ –ø—É—Å—Ç—ã—Ö –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø—É—Å—Ç–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –ü—Ä–æ–ø—É—Å–∫.")
                continue
            
            state = torch.FloatTensor(sequence).to(agent.device)
            action = agent.select_action(state)
            
            # –ü—Ä–∏–º–µ—Ä –Ω–∞–≥—Ä–∞–¥—ã
            reward = 1.0  
            done = False  

            policy, value = agent.model(state.unsqueeze(0))
            log_prob = torch.log(policy.squeeze(0)[action])
            
            log_probs.append(log_prob)
            values.append(value)
            rewards.append(reward)
            dones.append(done)
        
        agent.update(rewards, log_probs, values, dones)
        
        if (epoch + 1) % config['metrics']['log_interval'] == 0:
            logger.info(f"–≠–ø–æ—Ö–∞ {epoch + 1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    agent.save_model(config['general']['save_model_path'])
    logger.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")


