import torch
import torch.nn as nn
import torch.optim as optim
import yaml
from loguru import logger

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logger.add(config['general']['training_log_path'], rotation="1 MB", level=config['general']['log_level'])


# ==========================
# ü§ñ –ú–û–î–ï–õ–¨ –î–õ–Ø –ú–ï–¢–ê-–û–ë–£–ß–ï–ù–ò–Ø
# ==========================
class MAML:
    def __init__(self, model, inner_lr, outer_lr, adaptation_steps):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MAML.

        Args:
            model (nn.Module): –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
            inner_lr (float): –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è.
            outer_lr (float): –í–Ω–µ—à–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è.
            adaptation_steps (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏.
        """
        self.model = model
        self.inner_lr = inner_lr
        self.outer_lr = outer_lr
        self.adaptation_steps = adaptation_steps
        self.outer_optimizer = optim.Adam(self.model.parameters(), lr=self.outer_lr)
    
    def adapt(self, task_data):
        """
        –ê–¥–∞–ø—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤–æ–π –∑–∞–¥–∞—á–µ.

        Args:
            task_data (tuple): –î–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ (inputs, targets).

        Returns:
            nn.Module: –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å.
        """
        inputs, targets = task_data
        model_copy = self._clone_model()
        inner_optimizer = optim.SGD(model_copy.parameters(), lr=self.inner_lr)
        
        for step in range(self.adaptation_steps):
            predictions = model_copy(inputs)
            loss = nn.CrossEntropyLoss()(predictions, targets)
            inner_optimizer.zero_grad()
            loss.backward()
            inner_optimizer.step()
        
        return model_copy
    
    def meta_update(self, tasks):
        """
        –ú–µ—Ç–∞-–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–¥–∞—á.

        Args:
            tasks (list): –°–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
        """
        meta_loss = 0.0
        self.outer_optimizer.zero_grad()
        
        for task_data in tasks:
            inputs, targets = task_data
            adapted_model = self.adapt(task_data)
            predictions = adapted_model(inputs)
            loss = nn.CrossEntropyLoss()(predictions, targets)
            meta_loss += loss
        
        meta_loss /= len(tasks)
        meta_loss.backward()
        self.outer_optimizer.step()
        
        logger.info(f"üîÑ MAML ‚Äî –ú–µ—Ç–∞-–ª–æ—Å—Å: {meta_loss.item():.4f}")
    
    def _clone_model(self):
        """
        –°–æ–∑–¥–∞–µ—Ç –∫–æ–ø–∏—é –º–æ–¥–µ–ª–∏ —Å —Ç–µ–º–∏ –∂–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
        """
        clone = type(self.model)().to(next(self.model.parameters()).device)
        clone.load_state_dict(self.model.state_dict())
        return clone
